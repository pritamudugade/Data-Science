{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmTGPfRVMNLmOyF0FGL/23",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pritamudugade/Data-Science/blob/main/Neural_network_custom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HnUCHT9wpQzk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from tensorflow.keras import layers, Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Define the size of the synthetic images\n",
        "image_height = 256\n",
        "image_width = 256\n",
        "\n",
        "# Define the number of classes\n",
        "num_classes = 5\n",
        "\n",
        "# Create an empty list to store the synthetic dataset\n",
        "synthetic_dataset = []\n",
        "\n",
        "# Path to your annotation folder\n",
        "annotation_folder = \"/content/drive/MyDrive/IntruderDetectionYOLOv8.v1i.yolov7pytorch/train/labels\"\n",
        "\n",
        "# Load real images from your dataset\n",
        "image_paths = glob.glob(\"/content/drive/MyDrive/IntruderDetectionYOLOv8.v1i.yolov7pytorch/train/*.jpg\")\n",
        "\n",
        "# Generate the synthetic dataset\n",
        "for image_path in image_paths:\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(\"Error loading image:\", image_path)\n",
        "        continue\n",
        "    image = cv2.resize(image, (image_width, image_height))  # Resize the image to the desired dimensions\n",
        "\n",
        "    # Load annotation file for the current image\n",
        "    annotation_file = os.path.join(annotation_folder, os.path.basename(image_path).replace(\".jpg\", \".txt\"))\n",
        "\n",
        "    # Read bounding box coordinates and class labels from the annotation file\n",
        "    bounding_boxes = []\n",
        "    class_labels = []\n",
        "    with open(annotation_file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            parts = line.strip().split()\n",
        "            class_label = int(parts[0])  # Assuming class label is the first value in the annotation line\n",
        "            x_center = float(parts[1]) * image_width\n",
        "            y_center = float(parts[2]) * image_height\n",
        "            width = float(parts[3]) * image_width\n",
        "            height = float(parts[4]) * image_height\n",
        "            x1 = int(x_center - width / 2)\n",
        "            y1 = int(y_center - height / 2)\n",
        "            bounding_boxes.append([x1, y1, width, height])\n",
        "            class_labels.append(class_label)\n",
        "\n",
        "    # Append the image, bounding boxes, and class labels to the synthetic dataset\n",
        "    synthetic_dataset.append({\"image\": image, \"bounding_boxes\": bounding_boxes, \"class_labels\": class_labels})\n",
        "\n",
        "# Convert the synthetic dataset to NumPy arrays\n",
        "X = np.array([data[\"image\"] for data in synthetic_dataset])\n",
        "y_bboxes = np.array([data[\"bounding_boxes\"] for data in synthetic_dataset])\n",
        "y_labels = np.array([data[\"class_labels\"] for data in synthetic_dataset])\n",
        "\n",
        "# Now you can use X, y_bboxes, and y_labels for training your object detection model\n",
        "# Make sure to preprocess the data and annotations before feeding them to the model\n"
      ],
      "metadata": {
        "id": "j2hiUqfNq31e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN-based object detection model\n",
        "class ObjectDetectionModel(Model):\n",
        "    def __init__(self, num_classes, num_anchors):\n",
        "        super(ObjectDetectionModel, self).__init__()\n",
        "        # Define the layers for your model\n",
        "        # Replace the following lines with your custom CNN architecture for object detection\n",
        "        self.conv1 = layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')\n",
        "        self.pool1 = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "        self.conv2 = layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')\n",
        "        self.pool2 = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "        self.flatten = layers.Flatten()\n",
        "        self.fc1 = layers.Dense(128, activation='relu')\n",
        "        self.output_layer = layers.Dense(num_classes + 4 * num_anchors, activation='linear')\n",
        "\n",
        "    def call(self, x):\n",
        "        # Define the forward pass of your model\n",
        "        # Replace the following lines with your custom model architecture\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "MEW1lnd9q9tQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the model\n",
        "num_classes = 4  # Replace with the number of classes in your dataset\n",
        "num_anchors = 4   # Replace with the number of anchors used in the detection\n",
        "model = ObjectDetectionModel(num_classes=num_classes, num_anchors=num_anchors)\n",
        "model = ObjectDetectionModel(num_classes, num_anchors)\n"
      ],
      "metadata": {
        "id": "DqxlxTWDrM5m"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function and optimizer\n",
        "loss_function = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "metadata": {
        "id": "nqbPSme-rV4e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def custom_object_detection_loss(y_true, y_pred):\n",
        "    # Split y_true into classification and regression targets\n",
        "    true_classes = y_true[:, :num_classes]\n",
        "    true_boxes = y_true[:, num_classes:]\n",
        "\n",
        "    # Split y_pred into predicted classes and regression predictions\n",
        "    pred_classes = y_pred[:, :num_classes]\n",
        "    pred_boxes = y_pred[:, num_classes:]\n",
        "\n",
        "    # Classification loss (example: categorical cross-entropy)\n",
        "    class_loss = tf.keras.losses.CategoricalCrossentropy()(true_classes, pred_classes)\n",
        "\n",
        "    # Regression loss (example: smooth L1 loss for bounding box regression)\n",
        "    reg_loss = tf.keras.losses.Huber()(true_boxes, pred_boxes)\n",
        "\n",
        "    # Combine the losses with appropriate weights\n",
        "    total_loss = class_loss + reg_loss\n",
        "\n",
        "    return total_loss\n"
      ],
      "metadata": {
        "id": "dJpHhp61xlHO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        batch_images = X[i:i+batch_size]\n",
        "        batch_labels = y[i:i+batch_size]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(batch_images)\n",
        "            loss = loss_function(batch_labels, predictions)\n",
        "\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        print(f\"Batch {i//batch_size+1}/{len(X)//batch_size}, Loss: {loss.numpy()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QprtthgrY5O",
        "outputId": "75ddb607-51e4-42a6-d1f3-fce26f6c017c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "Epoch 2/50\n",
            "Epoch 3/50\n",
            "Epoch 4/50\n",
            "Epoch 5/50\n",
            "Epoch 6/50\n",
            "Epoch 7/50\n",
            "Epoch 8/50\n",
            "Epoch 9/50\n",
            "Epoch 10/50\n",
            "Epoch 11/50\n",
            "Epoch 12/50\n",
            "Epoch 13/50\n",
            "Epoch 14/50\n",
            "Epoch 15/50\n",
            "Epoch 16/50\n",
            "Epoch 17/50\n",
            "Epoch 18/50\n",
            "Epoch 19/50\n",
            "Epoch 20/50\n",
            "Epoch 21/50\n",
            "Epoch 22/50\n",
            "Epoch 23/50\n",
            "Epoch 24/50\n",
            "Epoch 25/50\n",
            "Epoch 26/50\n",
            "Epoch 27/50\n",
            "Epoch 28/50\n",
            "Epoch 29/50\n",
            "Epoch 30/50\n",
            "Epoch 31/50\n",
            "Epoch 32/50\n",
            "Epoch 33/50\n",
            "Epoch 34/50\n",
            "Epoch 35/50\n",
            "Epoch 36/50\n",
            "Epoch 37/50\n",
            "Epoch 38/50\n",
            "Epoch 39/50\n",
            "Epoch 40/50\n",
            "Epoch 41/50\n",
            "Epoch 42/50\n",
            "Epoch 43/50\n",
            "Epoch 44/50\n",
            "Epoch 45/50\n",
            "Epoch 46/50\n",
            "Epoch 47/50\n",
            "Epoch 48/50\n",
            "Epoch 49/50\n",
            "Epoch 50/50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model as a SavedModel\n",
        "model.save('object_detection_model', save_format='tf')\n",
        "print(\"Trained model saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "BHUUmvyUx8Eg",
        "outputId": "244dfbb4-f65c-42f5-be4d-79f9d3359a35"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.ObjectDetectionModel object at 0x7db65c9b07c0>, because it is not built.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-515dae535792>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save the trained model as a SavedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object_detection_model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trained model saved.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/saving/legacy/saving_utils.py\u001b[0m in \u001b[0;36mraise_model_input_error\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# If the model is not a `Sequential`, it is intended to be a subclassed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;34mf\"Model {model} cannot be saved either because the input shape is not \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;34m\"available or because the forward pass of the model is not defined.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Model <__main__.ObjectDetectionModel object at 0x7db65c9b07c0> cannot be saved either because the input shape is not available or because the forward pass of the model is not defined.To define a forward pass, please override `Model.call()`. To specify an input shape, either call `build(input_shape)` directly, or call the model on actual data using `Model()`, `Model.fit()`, or `Model.predict()`. If you have a custom training step, please make sure to invoke the forward pass in train step through `Model.__call__`, i.e. `model(inputs)`, as opposed to `model.call()`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('object_detection_model_weights.h5')\n"
      ],
      "metadata": {
        "id": "U8ypRDBjuXLm"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}